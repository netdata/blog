---
title: "What is edge monitoring and why does real-time health and performance matter?"
date: 2019-06-26
summary: ""
author: "Joel Hans"
cover: ""
tags: ["edge computing", "edge monitoring", "IoT"]
categories: []
draft: true
---

Enough with Internet of Things (IoT) deployments full of "dumb" devices that can only collect data and respond from instructions from the cloud. Enter edge computing.

Edge computing is the practice of collecting and analyzing data precisely where it is collected, on the far edges of the network, rather than streaming it back to a centralized location. And edge monitoring presents an opportunity to make remote devices smarter and give system administrators and DevOps engineers more tools to respond even faster during anomalies, slowdowns, and outages.

Together, edge computing and monitoring will help enterprises worry less about how they're going to stream enormous amounts of real-time edge device data back to a cloud computing environment, and how much it's going to cost them, and focus more on making their infrastructure faster and resilient. 

Maybe even extraordinary.

## The complexity of edge computing

Edge computing can take on many forms. A Tesla Model 3 driving in Autopilot mode is a type of mobile edge computing solution, since it's collecting and processing data from its sensors to make decisions about avoiding other vehicles and safely changing lanes. Edge devices also appear in remote or rugged industrial environments, like oil rigs and manufacturing plant floors. Or, they can be devices you find at home, like an Amazon Alexa or Google Home Mini.

{{< figure src="/img/edge-monitoring-netdata_googlehome.jpg" alt="A charcoal Google Home Mini." position="center" style="border-radius: 4px;" caption="A charcoal Google Home Mini. Photo by Bence Boros on Unsplash." captionPosition="center" >}}

Edge computing also represents a break from the recent trend of centralizing all data in one data center or cloud computing environment. Instead of streaming data toward that single location and relying on the great quantities of computing power, edge computing uses the computational power that's already there. Enterprises should be able to distribute processing and reduce their reliance on single points of failure with edge computing—if they do it right.

The ongoing debate about the merits of cloud vs. edge computing aside, there are a few truths that system administrators and DevOps engineers need to contend with. A recent [Allied Market Research report](https://www.marketsandmarkets.com/Market-Reports/edge-computing-market-133384090.html) predicts the glocal edge computing market will grow to $16.55 billion by 2025. That's up from $1.73 billion in 2017, for a compound annual growth rate of nearly 33%.

> By 2025, 75% of data generated by enterprises will be created and processes outside of a traditional data center.

Or to put that growth in the context of data, [Gartner predicts](https://www.gartner.com/smarterwithgartner/what-edge-computing-means-for-infrastructure-and-operations-leaders/) that by 2025, 75% of data generated by enterprises will be created and processes outside of a traditional data center or cloud computing environment. That's up from 10% right now.

"Organizations that have embarked on a digital business journey have realized that a more decentralized approach is required to address digital business infrastructure requirements," says Santhosh Rao, senior research director at Gartner. "As the volume and velocity of data increases, so too does the inefficiency of streaming all this information to a cloud or data center for processing."

## How edge monitoring for health and performance fits in

There are two big issues with most monitoring solutions. They're `1)` too complex for the edge devices themselves to *process* the data being collected at any given moment, and thus `2)` are reliant on centralized compute power and storage for keeping data.

But enterprises with IoT networks can't just ignore health and performance monitoring on the edge. Without it, there's almost no visibility into how these remote devices are working. Are they online? Are their sensors working properly? Are they collecting the right information at the right granularity?

Given how important edge devices can be to the health of an infrastructure, or even the health of people working in difficult environments, there's no room for second guessing. And we're trying to avoid all these "dumb" devices, after all.

That means system administrators and network managers need strategies and solutions that answer the following questions, plus many others:

**Where and how does analysis happen?** As I mentioned earlier, a lot of edge monitoring solutions aren't actually performing analysis on the edge. Instead, they're aggregating streaming data from remote edge devices in a centralized location and using that compute power to make insights about the data. But, this could be creating latency between the collection an anomalous event and the response, no matter how quickly the network can manage to stream and analyze .

**How (and how quickly) can we respond to that analysis?** Speaking of latency—enterprises need to be able to respond as quickly as possible, and with as little human intervention as possible. The worst-case scenario is that a technician has to travel out to the anomalous device for a closer look. The best-case scenario is something we like to call *self-healing*, but we'll cover that in more detail in a moment. Usually enterprises fall somewhere between those two ends of the spectrum but wish they could respond even faster and easier.

**How much will this cost?** Streaming and storing large volumes of data from a large edge network in one cloud computing environment isn't going to come cheap. And that's just for storing data—any monitoring and analysis solutions will come at an additional cost. In many cases, these costs become so prohibitive as to restrict the granularity of data collection, which can let anomalies go unnoticed.

## Netdata's commitment to real-time edge monitoring

Netdata's [open-source monitoring agent](https://github.com/netdata/netdata) takes a different approach than many others to health and performance monitoring on the edge. It's distributed, extremely lightweight, highly efficient, and infinitely scalable.

And it's pretty, too.

{{< figure src="/img/edge-monitoring-netdata_visualization.gif" alt="An animated gif of the Netdata web dashboard" position="center" style="border-radius: 4px;" caption="The Netdata web dashboard." captionPosition="center" >}}

Let's see how Netdata answers those three essential questions.

### Where and how does analysis happen?

Even without configuration, Netdata only needs 1% CPU utilization on a single co, 25MB of RAM, and no storage to collect thousands of metrics with real-time granularity. While others are capable of collecting metrics every 10 seconds, or even less frequently, Netdata has 1 second of granularity, meaning you'll never miss essential information about an ongoing anomaly.

That low utilization means Netdata can run, and collect thousands of metrics, on even the lowest-power edge devices.

Because Netdata is distributed by design, there's no need to stream or centralize any of the edge monitoring data in order to perform analysis. If a Netdata agent detects anomalous sensor data, it will instantly trigger an alarm without needing to stream or process data in a centralized location.

### How do we respond to that analysis?

Netdata is more than distributed, too—it's customizable to a variety of IoT infrastructures. 


Netdata also runs 

A distributed edge monitoring solution like Netdata also creates opportunities for *self-healing* infrastructures with edge devices that are capable of collecting data, analyzing it, and responding accordingly with a 


 The device itself can then trigger processes, entirely on the edge, to resolve the situation—no sysadmins required.

---

https://docs.netdata.cloud/docs/netdata-for-iot/
https://docs.netdata.cloud/docs/performance/
NetData only needs 1% of 1 core, 1MB RAM, no storage

Perfect for small IoT devices that you don't want to overwhelm with a more complex monitoring agent

Stream data straight to a master server with the dashboard to see the health of all devices in one place


---

Edge monitoring presents an opportunity for enterprises to stop worrying about how they're going to stream enormous amounts of real-time edge device data back to a cloud computing environment, and how much it's going to cost them. It's an opportunity to make remote devices smarter and give system administrators and DevOps engineers more tools to respond even faster during anomalies, slowdowns, and outages.


Enough with Internet of Things (IoT) deployments with devices that need to send every bit of data back to a centralized cloud computing environment. The truth is that with the rapid deployment of new IoT devices, and more complex infrastructures, edge computing and edge monitoring are an opportunity to be more efficient, act faster, and take advantage of the power of edge devices.

Edge monitoring aims to bring real-time decision-making opportunities to remote devices 

Edge monitoring promises to change that less-than-real-time monitoring reaction speed. By bringing metric collection *and* analysis to the edge devices themselves, edge monitoring will give system adminisrators an alterative to sending huge volumes of data from thousands of remote devices, analyzing that information in a central location, and hoping to respond fast enough during anomalies or outages.